name: export-whisper-to-onnx

on:
  workflow_dispatch:

concurrency:
  group: release-whisper-${{ github.ref }}
  cancel-in-progress: true

jobs:
  release-whisper-models:
    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
    name: ${{ matrix.model }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [macos-latest]
        model: ["distil-medium.en", "distil-small.en",  "tiny.en", "base.en", "small.en", "medium.en", "tiny", "base", "small", "medium", "medium-aishell", "large", "large-v1", "large-v2", "large-v3", "distil-large-v2"]
        # model: ["large", "large-v1", "large-v2", "large-v3", "distil-large-v2"]
        python-version: ["3.8"]

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        shell: bash
        run: |
          python3 -m pip install torch==1.13.0 torchaudio==0.13.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html
          python3 -m pip install openai-whisper==20231117 onnxruntime onnx soundfile librosa

      - name: export ${{ matrix.model }}
        shell: bash
        run: |
          cd scripts/whisper
          model=${{ matrix.model }}
          echo "model: $model"
          if [[ $model == distil-medium.en ]]; then
            wget -q -O distil-medium-en-original-model.bin https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/original-model.bin
            ls -lh
          elif [[ $model == distil-large-v2 ]]; then
            wget -q -O distil-large-v2-original-model.bin https://huggingface.co/distil-whisper/distil-large-v2/resolve/main/original-model.bin
            ls -lh
          elif [[ $model == distil-small.en ]]; then
            wget -q -O distil-small-en-original-model.bin https://huggingface.co/distil-whisper/distil-small.en/resolve/main/original-model.bin
            ls -lh
          elif [[ $model == medium-aishell ]]; then
            wget -q -O medium-aishell.pt https://huggingface.co/yuekai/icefall_asr_aishell_whisper/resolve/main/exp_medium/whisper-medium-aishell1-epoch-10-avg-4.pt
            ls -lh
          fi
          python3 ./export-onnx.py --model ${{ matrix.model }}
          # python3 -m onnxruntime.tools.convert_onnx_models_to_ort --optimization_style=Fixed ./
          #


          ls -lh

          ls -lh ~/.cache/whisper || true
          ls -lh distil*original-model.bin || true
          rm -rf ~/.cache/whisper
          rm -f distil*original-model.bin
          rm -f medium-aishell.pt

          src=sherpa-onnx-whisper-${{ matrix.model }}

          cd ..
          mkdir $src
          mv -v whisper/$model* $src/

          echo "------------------------------"

          cd $src
          du -h -d1 .
          ls -lh
          mkdir -p test_wavs
          cd test_wavs
          wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/0.wav
          wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/1.wav
          wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/8k.wav
          wget -q https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/trans.txt
          cd ../..
          mv $src ../
          echo "pwd: $PWD"

          cd ../
          echo "--------------------"
          ls -lh
          ls -lh $src
          echo "--------------------"

          if [[ $model == medium-aishell ]]; then
            ls -lh *.onnx # the float32 onnx model for medium-aishell is too large to be uploaded to GitHub
            mkdir -p bak
            mv -v $src/$model-encoder.onnx ./bak
            mv -v $src/$model-decoder.onnx ./bak
            ls -lh $src

            tar cvjf $src.tar.bz2 $src
            mv -v ./bak/* $src/
            rm -rf bak
          elif [[ -f $src/$model-encoder.weights ]]; then
            # we only publish int8 models to GitHub for large Whisper models
            mkdir -p bak
            mv -v $src/*weights ./bak
            mv -v $src/$model-encoder.onnx ./bak
            mv -v $src/$model-decoder.onnx ./bak
            ls -lh $src

            tar cvjf $src.tar.bz2 $src
            mv -v ./bak/* $src/
            rm -rf bak
          else
            tar cvjf $src.tar.bz2 $src
          fi

          ls -lh *.tar.bz2

      - name: Release
        uses: svenstaro/upload-release-action@v2
        with:
          file_glob: true
          file: ./*.tar*
          overwrite: true
          repo_name: k2-fsa/sherpa-onnx
          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
          tag: asr-models

      - name: Publish ${{ matrix.model }} to huggingface
        shell: bash
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          src=sherpa-onnx-whisper-${{ matrix.model }}

          git config --global user.email "csukuangfj@gmail.com"
          git config --global user.name "Fangjun Kuang"

          export GIT_CLONE_PROTECTION_ACTIVE=false

          export GIT_LFS_SKIP_SMUDGE=1

          git clone https://huggingface.co/csukuangfj/sherpa-onnx-whisper-${{ matrix.model }} huggingface

          rm -rf huggingface/*

          cp -av $src/* ./huggingface/

          cd huggingface

          git status
          ls -lh
          git lfs track "*onnx*"
          git lfs track "*weights*"

          git add .
          git commit -m "upload ${{ matrix.model }}"
          git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-whisper-${{ matrix.model }} main

      - name: Test float32 ${{ matrix.model }}
        shell: bash
        run: |
          python3 -m pip install kaldi-native-fbank
          model=${{ matrix.model }}
          src=sherpa-onnx-whisper-$model
          time python3 scripts/whisper/test.py \
            --encoder $src/$model-encoder.onnx \
            --decoder $src/$model-decoder.onnx \
            --tokens $src/$model-tokens.txt \
            $src/test_wavs/0.wav

      - name: Test int8 ${{ matrix.model }}
        shell: bash
        run: |
          model=${{ matrix.model }}
          src=sherpa-onnx-whisper-$model
          time python3 scripts/whisper/test.py \
            --encoder $src/$model-encoder.int8.onnx \
            --decoder $src/$model-decoder.int8.onnx \
            --tokens $src/$model-tokens.txt \
            $src/test_wavs/0.wav
